> If $q_t(s_t, a_t)$ is estimated by TD learning, the corresponding algorithms are usually called actor-critic.

## QAC

![image-20240204182025888](.static/image-20240204182025888.png)



 



## Actor-Critic

$V_{\pi}(s)=\underset{{\color{red}{a}}}{\sum}\pi({\color{red}{a}}|s)\cdot Q_{\pi}(s,{\color{red}{a}})\approx\underset{{\color{red}{a}}}{\sum}\pi({\color{red}{a}}|s;\mathbf{\theta})\cdot q(s,{\color{red}{a}};\mathbf{w})$

Two neural nets for Value Network and Policy Network. 

- **Policy network (actor) && Value network (critic)**

During training:

- Agent is controlled by policy network (actor)

- Value network $q$ (critic) provides the actor with supervision.

- Update the policy network (actor) by **policy gradient**.
- Update the value network (critic) by **TD learning**.

After training:

- Agent is controlled by policy network (actor)
- Value network $q$ (critic) will not be used.

Training:

![image-20231129135645473](.static/image-20231129135645473.png)

For last step, $\mathbf{\theta}_{t+1}=\mathbf{\theta}_t+\mathbf{\beta}\cdot\delta_t\cdot\mathbf{d}_{\theta,t}$ in policy gradient with baseline.
