> If $q_t(s_t, a_t)$ is estimated by TD learning, the corresponding algorithms are usually called actor-critic.

## QAC

![image-20240204182025888](.static/image-20240204182025888.png)

## Advantage actor-critic (A2C)

**Baseline invariance**

 $\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)\right]=\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_\theta\ln\pi(A|S,\theta_t)(q_\pi(S,A)-b(S))\right]$

Proof: 

![image-20240204212313547](.static/image-20240204212313547.png)

The baseline is useful because it can reduce the **approximation variance** when we use samples to approximate the true gradient. We can select the suboptimal baseline: 
$$
b(s)=\mathbb{E}_{A\sim\pi}[q_\pi(s,A)=v_\pi(s)]
$$

![image-20240204213351693](.static/image-20240204213351693.png)

- Only need to use a single neural network to represent $v_\pi(s)$.

- The policy $Ï€(\theta_t)$ is stochastic and hence exploratory. Therefore, it can be directly used to generate experience samples without relying on








## Actor-Critic

$V_{\pi}(s)=\underset{{\color{red}{a}}}{\sum}\pi({\color{red}{a}}|s)\cdot Q_{\pi}(s,{\color{red}{a}})\approx\underset{{\color{red}{a}}}{\sum}\pi({\color{red}{a}}|s;\mathbf{\theta})\cdot q(s,{\color{red}{a}};\mathbf{w})$

Two neural nets for Value Network and Policy Network. 

- **Policy network (actor) && Value network (critic)**

During training:

- Agent is controlled by policy network (actor)

- Value network $q$ (critic) provides the actor with supervision.

- Update the policy network (actor) by **policy gradient**.
- Update the value network (critic) by **TD learning**.

After training:

- Agent is controlled by policy network (actor)
- Value network $q$ (critic) will not be used.

Training:

![image-20231129135645473](.static/image-20231129135645473.png)

For last step, $\mathbf{\theta}_{t+1}=\mathbf{\theta}_t+\mathbf{\beta}\cdot\delta_t\cdot\mathbf{d}_{\theta,t}$ in policy gradient with baseline.
